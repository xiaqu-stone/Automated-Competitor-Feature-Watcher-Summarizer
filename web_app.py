from flask import Flask, render_template, jsonify, request, Response
import threading
import queue
import time
import json
from datetime import datetime
import sys
import io
from contextlib import redirect_stdout
import os

# Import our existing MVP logic
from mvp_demo import (
    load_processed_urls, save_processed_url, get_article_urls,
    get_article_text, analyze_text, display_results
)

app = Flask(__name__)

# Global state management
app_state = {
    'status': 'ready',  # ready, running, completed, error
    'progress': 0,
    'total_articles': 0,
    'processed_articles': 0,
    'current_task': '',
    'results': [],
    'logs': [],
    'start_time': None,
    'end_time': None
}

# Thread-safe queue for real-time logs
log_queue = queue.Queue()

class LogCapture:
    """Capture print statements and send to web interface"""
    def __init__(self):
        self.logs = []
    
    def write(self, text):
        if text.strip():
            timestamp = datetime.now().strftime("%H:%M:%S")
            log_entry = f"[{timestamp}] {text.strip()}"
            self.logs.append(log_entry)
            log_queue.put(log_entry)
            # Also print to console for debugging
            print(text, end='')
    
    def flush(self):
        pass

def run_analysis_task():
    """Background task to run the ACFWS analysis"""
    global app_state
    
    try:
        app_state['status'] = 'running'
        app_state['start_time'] = datetime.now()
        app_state['current_task'] = 'åˆå§‹åŒ–åˆ†æä»»åŠ¡...'
        
        # Capture stdout to get script output
        log_capture = LogCapture()
        
        with redirect_stdout(log_capture):
            print("ğŸš€ å¼€å§‹åˆ†æç«å“åŠŸèƒ½...")
            
            # Load processed URLs
            print("ğŸ“‚ åŠ è½½å·²å¤„ç†çš„æ–‡ç« ç¼“å­˜...")
            processed_urls = load_processed_urls()
            app_state['current_task'] = 'åŠ è½½ç¼“å­˜å®Œæˆ'
            
            # Get article URLs
            print("ğŸ” è·å–æ–‡ç« URLåˆ—è¡¨...")
            all_urls = get_article_urls()
            app_state['total_articles'] = len(all_urls)
            app_state['current_task'] = f'å‘ç° {len(all_urls)} ç¯‡æ–‡ç« '
            
            # Filter new URLs
            new_urls = [url for url in all_urls if url not in processed_urls]
            print(f"ğŸ“Š å‘ç° {len(new_urls)} ç¯‡æ–°æ–‡ç« éœ€è¦åˆ†æ")
            
            if not new_urls:
                print("âœ… æ‰€æœ‰æ–‡ç« éƒ½å·²åˆ†æè¿‡ï¼Œæ— éœ€é‡å¤å¤„ç†")
                app_state['status'] = 'completed'
                app_state['end_time'] = datetime.now()
                return
            
            # Process each new article
            results = []
            for i, url in enumerate(new_urls):
                app_state['processed_articles'] = i
                app_state['progress'] = int((i / len(new_urls)) * 100)
                app_state['current_task'] = f'åˆ†ææ–‡ç«  {i+1}/{len(new_urls)}'
                
                print(f"\nğŸ“– æ­£åœ¨å¤„ç†æ–‡ç«  {i+1}/{len(new_urls)}: {url}")
                
                # Get article content
                print("  ğŸ”„ è·å–æ–‡ç« å†…å®¹...")
                article_text = get_article_text(url)
                
                if article_text and len(article_text.strip()) > 100:
                    print("  ğŸ¤– ä½¿ç”¨AIåˆ†æå†…å®¹...")
                    analysis = analyze_text(article_text)
                    
                    if analysis:
                        result = {
                            'url': url,
                            'analysis': analysis,
                            'timestamp': datetime.now().isoformat(),
                            'article_preview': article_text[:200] + "..." if len(article_text) > 200 else article_text
                        }
                        results.append(result)
                        
                        # Display results (this will be captured in logs)
                        display_results(analysis)
                        
                        # Save to cache
                        save_processed_url(url)
                        print(f"  âœ… æ–‡ç« åˆ†æå®Œæˆå¹¶ä¿å­˜åˆ°ç¼“å­˜")
                    else:
                        print(f"  âŒ AIåˆ†æå¤±è´¥")
                else:
                    print(f"  âš ï¸  æ–‡ç« å†…å®¹è·å–å¤±è´¥æˆ–å†…å®¹è¿‡çŸ­")
                
                time.sleep(0.5)  # Small delay for demo effect
            
            app_state['results'] = results
            app_state['processed_articles'] = len(new_urls)
            app_state['progress'] = 100
            app_state['status'] = 'completed'
            app_state['end_time'] = datetime.now()
            
            print(f"\nğŸ‰ åˆ†æå®Œæˆ! å…±å¤„ç† {len(new_urls)} ç¯‡æ–‡ç« ï¼Œå‘ç° {len(results)} ä¸ªåˆ†æç»“æœ")
            
    except Exception as e:
        app_state['status'] = 'error'
        app_state['current_task'] = f'é”™è¯¯: {str(e)}'
        app_state['end_time'] = datetime.now()
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")

@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template('index.html', state=app_state)

@app.route('/start', methods=['POST'])
def start_analysis():
    """å¯åŠ¨åˆ†æä»»åŠ¡"""
    if app_state['status'] in ['running']:
        return jsonify({'error': 'åˆ†ææ­£åœ¨è¿›è¡Œä¸­'}), 400
    
    # Reset state
    app_state.update({
        'status': 'ready',
        'progress': 0,
        'total_articles': 0,
        'processed_articles': 0,
        'current_task': '',
        'results': [],
        'logs': [],
        'start_time': None,
        'end_time': None
    })
    
    # Clear log queue
    while not log_queue.empty():
        log_queue.get()
    
    # Start background task
    thread = threading.Thread(target=run_analysis_task)
    thread.daemon = True
    thread.start()
    
    return jsonify({'message': 'åˆ†æä»»åŠ¡å·²å¯åŠ¨'})

@app.route('/status')
def get_status():
    """è·å–å½“å‰çŠ¶æ€"""
    return jsonify(app_state)

@app.route('/logs')
def stream_logs():
    """å®æ—¶æ—¥å¿—æµ"""
    def generate():
        while True:
            try:
                # Get log from queue with timeout
                log = log_queue.get(timeout=1)
                yield f"data: {json.dumps({'log': log})}\n\n"
            except queue.Empty:
                # Send heartbeat
                yield f"data: {json.dumps({'heartbeat': True})}\n\n"
                
                # If analysis is complete, stop streaming
                if app_state['status'] in ['completed', 'error']:
                    break
    
    return Response(generate(), mimetype='text/event-stream',
                   headers={'Cache-Control': 'no-cache'})

@app.route('/monitor')
def monitor():
    """ç›‘æ§é¡µé¢"""
    return render_template('monitor.html', state=app_state)

@app.route('/results')
def results():
    """ç»“æœé¡µé¢"""
    return render_template('results.html', state=app_state)

if __name__ == '__main__':
    print("ğŸš€ ACFWS Webæ¼”ç¤ºå¯åŠ¨ä¸­...")
    print("ğŸ“± è¯·åœ¨æµè§ˆå™¨ä¸­è®¿é—®: http://localhost:5000")
    app.run(debug=True, host='0.0.0.0', port=5000) 